{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "In this Jupyter Notebook we'll dive further into the evaluation of deep learning models for medicine. Since the decisions we are going to take with the model's prediction  are of  high impact, we care about understanding exactly when a model works on a patient and when it does not. \n",
    "\n",
    "You might be wondering, is in it accuracy enough to tell if our model is good or bad? The short answer is no. Take for example this dataset and these two models: \n",
    "\n",
    "![accuracy](utils\\acuraccy.PNG)\n",
    "\n",
    " The first model is predicting that all patients are negative (normal). On the other hand, the second model is predicting some patients are postitive but is making some mistakes.\n",
    " \n",
    "In fact the second model is better because it's _learning_, but both models have an accuracy of 0.80. This is the moment when we realize that accuracy might be actually disguising the model's performance. Lets analyze more metrics.\n",
    "\n",
    "### Deriving Specificity and Sensitivity from Accuracy using Probability:\n",
    "\n",
    "\n",
    "You could thing of accuracy as a probability, remember that accuracy ranges from [0,1] so it is compliant with all the axioms of probability\n",
    "\n",
    "$$ Accuracy =  P(Correct)  =  \\frac{Correct_{labels}}{Total_{labels}}\\; .....(1)$$\n",
    "\n",
    "Furthermore, the probability of being  correct is the intesection of a correct label with a correct prediciton:\n",
    "\n",
    "$$Accuracy =  P(correct \\cap disease ) + P(correct \\cap normal ) $$\n",
    "\n",
    "As a reminder, the law of conditional probability says that the probability of A and B is the probability of A given B times the probability of B.  Do not get cofussed with the third axiom of probability \n",
    "\n",
    "$ P( A \\cap B) = P(A | B)P(B) $\n",
    "\n",
    "Therefore:\n",
    "<br>\n",
    "$P(correct \\cap disease ) = P(correct|disease)P(disease) $\n",
    "<br>\n",
    "$P(correct \\cap nromal )  = P(correct|normal)P(normal) $\n",
    "\n",
    "$$Accuracy =  P(correct|disease)P(disease) +  P(correct|normal)P(normal) $$\n",
    "\n",
    "In the medical world if a patient has a diseas we say it is positive to that diseas, and if the patient does not have the disease we say the patient is negative to the disease so we could say that \n",
    "\n",
    "$$Accuracy =  P(+|disease)P(disease) +  P(-|normal)P(normal) $$\n",
    "\n",
    "We call:\n",
    "<br>\n",
    "$ P(+|disease) $  Sensitivty (true positive rate): The probabilty  the model predicts the patient has the disease  given the fact that the pateient does in fact has it \n",
    "<br>\n",
    "\n",
    "$ P(-|normal) $  Specificity (true negative rate): The probability the model predicts the patient is healthy given the fact that  the patient is in fact healthy \n",
    "\n",
    "$P(disease)$ : The probability of a patient having disease in a population is called the prevalence. The probability of being normal is simply one minus the probability of having disease or one minus the prevalence.\n",
    "\n",
    "We can also get the equivalence by the law of conditional probability:\n",
    "* **Sensitivity:**\n",
    "$$P(+|disease)= \\frac{+ \\cap disease}{P(disease)}$$\n",
    "* **Specificity:**\n",
    "$$P(-|normal)=  \\frac{- \\cap normal}{P(normal)}$$\n",
    "* **Prevalence:**\n",
    "$$P(disease) = \\frac{disease_{patients}}{Total_{patients}}$$\n",
    "\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$ Acuracy = Sensitiviy*prevalence  + Specificity*(1-prevalence)\\;....(2) $$\n",
    "\n",
    "This equation allows us to see accuracy as a weighted average of sensitivity and specificity. The weight associated with the sensitivity is the prevalence, and the weight associated with the specificity is one minus the prevalence. This equation also allows us to find out any of these quantities given the other three quantities. Let's see this using an example.\n",
    "\n",
    "![example_accuracy](utils\\example_accuracy.PNG)\n",
    "\n",
    "We can calcualte the accuracy of the model either using equation $(1)$ or $(2)$\n",
    "\n",
    "Using equation $(1)$:\n",
    "$$Acuracy=  \\frac{Correct_{labels}}{Total_{labels}}= \\frac{8}{10} = 0.8  $$ \n",
    "\n",
    "Using equation $(2)$:\n",
    "\n",
    "* **Sensitivity:**\n",
    "<br>\n",
    "$ P(+|disease)= \\frac{+ \\cap disease}{disease_{total}} \\frac{2}{3} =0.67$\n",
    "\n",
    "* **Specificity:**\n",
    "<br>\n",
    "$ P(-|normal)=  \\frac{- \\cap normal}{normal_{total}} = \\frac{6}{7}=0.86 $\n",
    "\n",
    "* **Prevalence:**\n",
    "<br>\n",
    "$P(disease) = \\frac{disease_{patients}}{Total_{patients}}=\\frac{3}{10} =0.3 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7999999999999999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sensitivity = 2/3\n",
    "Specificity = 6/7\n",
    "Prevalence = 3/10\n",
    "Accuracy = Sensitivity*Prevalence  + Specificity*(1-Prevalence)\n",
    "Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positve Predicctive Value \n",
    "Sensitivity tells us: Given we know a patient has a disease, what is the probability that the model predicts positive? In the clinic, the doctor using an AI model may be interested in a different question. And that is given the model predicts positive on a patient, what is the probability that they actually have the disease? This is called the positive predictive value or PPV of the model.\n",
    "\n",
    "Similarly, while specificity asks, what is the probability the model predicts negative, given a patient is normal? The doctor may be interested in knowing the probability that a patient is normal, given the model prediction is negative. This is called the negative predictive value or NPV of a model. Let's compute the PPV and NPV on an\n",
    "\n",
    "![ppvppn](utils\\ppn.PNG)\n",
    "\n",
    "* **Positive Predictive Value:**\n",
    "$$P(disease|+)= \\frac{+ \\cap disease}{P(normal)}$$\n",
    "* **Negative Predictive Value:**\n",
    "$$P(normal|-)=  \\frac{- \\cap normal}{P(disease)}$$\n",
    "\n",
    "For the previous model that would be:\n",
    "\n",
    "* **Positive Predictive Value:**\n",
    "$$P(disease|+)= \\frac{+ \\cap disease}{P(normal)} = $$\n",
    "* **Negative Predictive Value:**\n",
    "$$P(normal|-)=  \\frac{- \\cap normal}{P(disease)}$$\n",
    "\n",
    "![model3]( utils\\model3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confussion Matrix \n",
    "As you might expect al these metrics are related:\n",
    "![Confusion](utils\\confussion_matrix.PNG)\n",
    "\n",
    "![confussion2](utils\\confussion2.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_env",
   "language": "python",
   "name": "machine_learning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
